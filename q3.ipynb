{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def tokenize_and_build_vocab(data, max_sequence_length):\n",
    "    tokenized_data = [line.strip().split(\" \") for line in data]\n",
    "    vocabulary = ['<OOV>'] + list(set(token for line in tokenized_data for token in line))\n",
    "    integer_sequences_padded = np.zeros((len(tokenized_data), max_sequence_length))\n",
    "\n",
    "    for i in range(len(tokenized_data)):\n",
    "        for j in range(min(max_sequence_length, len(tokenized_data[i]))):\n",
    "            token = tokenized_data[i][j]\n",
    "            integer_sequences_padded[i, j] = vocabulary.index(token) if token in vocabulary else vocabulary.index('<OOV>')\n",
    "\n",
    "    return torch.from_numpy(integer_sequences_padded).long(), vocabulary\n",
    "\n",
    "with open(\"train.sources\") as f:\n",
    "    train_sources = f.readlines()\n",
    "\n",
    "with open(\"train.targets\") as f:\n",
    "    train_targets = f.readlines()\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "X_train, train_sources_vocabulary = tokenize_and_build_vocab(train_sources, MAX_SEQUENCE_LENGTH)\n",
    "Y_train, train_targets_vocabulary = tokenize_and_build_vocab(train_targets, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.2)\n",
    "\n",
    "train_data = TensorDataset(X_train, Y_train)\n",
    "test_data = TensorDataset(X_test, Y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=32)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        # Combine bidirectional outputs\n",
    "        hidden = (hidden[::2, :, :] + hidden[1::2, :, :]) / 2\n",
    "        cell = (cell[::2, :, :] + cell[1::2, :, :]) / 2\n",
    "\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_size * 3, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        nn.init.normal_(self.v.data, mean=0, std=1. / np.sqrt(self.v.size(0)))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_length = encoder_outputs.shape[0]\n",
    "        hidden = hidden.repeat(seq_length, 1, 1).transpose(0, 1)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)\n",
    "        energy = torch.tanh(self.attention(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attention = F.softmax(torch.sum(self.v * energy, dim=2), dim=1).unsqueeze(1)\n",
    "        return attention\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size + hidden_size * 2, hidden_size, num_layers, dropout=dropout)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell, encoder_outputs):\n",
    "        x = x.unsqueeze(0)\n",
    "        embedded = self.embedding(x)\n",
    "        attention = self.attention(hidden[-1], encoder_outputs)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)\n",
    "        weighted = torch.bmm(attention, encoder_outputs)\n",
    "        weighted = weighted.transpose(0, 1)\n",
    "        output, (hidden, cell) = self.lstm(torch.cat((embedded, weighted), dim=2), (hidden, cell))\n",
    "        prediction = self.fc(output.squeeze(0))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        seq_length = target.shape[0]\n",
    "        input_size = self.decoder.fc.out_features\n",
    "        outputs = torch.zeros(seq_length, batch_size, input_size).to(self.device)\n",
    "        encoder_outputs, hidden, cell = self.encoder(source)\n",
    "        x = target[0]\n",
    "        for i in range(1, seq_length):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell, encoder_outputs)\n",
    "            outputs[i] = output\n",
    "            best_guess = output.argmax(1)\n",
    "            x = target[i] if random.random() < teacher_forcing_ratio else best_guess\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 1\n",
    "clip = 1\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        source = batch[0].to(device)\n",
    "        target = batch[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(source, target)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        target = target[1:].view(-1)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(\"Epoch:\",epoch+1,\"Loss:\",epoch_loss/len(iterator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
